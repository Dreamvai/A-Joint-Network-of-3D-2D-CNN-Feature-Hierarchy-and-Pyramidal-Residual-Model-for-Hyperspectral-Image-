{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperspectral data shape:  (145, 145, 200)\n",
      "Label shape:  (145, 145)\n",
      "\n",
      "... ... PCA tranformation ... ...\n",
      "Data shape after PCA:  (145, 145, 32)\n",
      "\n",
      "... ... create data cubes ... ...\n",
      "Data cube X shape:  (10249, 15, 15, 32)\n",
      "Data cube y shape:  (10249,)\n",
      "\n",
      "... ... create train & test data ... ...\n",
      "Xtrain shape:  (1024, 15, 15, 32)\n",
      "Xtest  shape:  (9225, 15, 15, 32)\n",
      "before transpose: Xtrain shape:  (1024, 32, 15, 15)\n",
      "before transpose: Xtest  shape:  (9225, 32, 15, 15)\n",
      "[Epoch: 1]   [loss avg: 34.7228]   [current loss: 3.9795]\n",
      "[Epoch: 2]   [loss avg: 28.0304]   [current loss: 2.0274]\n",
      "[Epoch: 3]   [loss avg: 24.4079]   [current loss: 1.9864]\n",
      "[Epoch: 4]   [loss avg: 21.4352]   [current loss: 1.2495]\n",
      "[Epoch: 5]   [loss avg: 19.2614]   [current loss: 1.2514]\n",
      "[Epoch: 6]   [loss avg: 17.6774]   [current loss: 1.2229]\n",
      "[Epoch: 7]   [loss avg: 16.3597]   [current loss: 0.8708]\n",
      "[Epoch: 8]   [loss avg: 15.1696]   [current loss: 0.8303]\n",
      "[Epoch: 9]   [loss avg: 14.1721]   [current loss: 0.6941]\n",
      "[Epoch: 10]   [loss avg: 13.2961]   [current loss: 0.5963]\n",
      "[Epoch: 11]   [loss avg: 12.5477]   [current loss: 0.6204]\n",
      "[Epoch: 12]   [loss avg: 11.8711]   [current loss: 0.6156]\n",
      "[Epoch: 13]   [loss avg: 11.3234]   [current loss: 0.5947]\n",
      "[Epoch: 14]   [loss avg: 10.7730]   [current loss: 0.4258]\n",
      "[Epoch: 15]   [loss avg: 10.2794]   [current loss: 0.5328]\n",
      "[Epoch: 16]   [loss avg: 9.8420]   [current loss: 0.3117]\n",
      "[Epoch: 17]   [loss avg: 9.4221]   [current loss: 0.3200]\n",
      "[Epoch: 18]   [loss avg: 9.0472]   [current loss: 0.2891]\n",
      "[Epoch: 19]   [loss avg: 8.6882]   [current loss: 0.3384]\n",
      "[Epoch: 20]   [loss avg: 8.3574]   [current loss: 0.2633]\n",
      "[Epoch: 21]   [loss avg: 8.0427]   [current loss: 0.2224]\n",
      "[Epoch: 22]   [loss avg: 7.7498]   [current loss: 0.2085]\n",
      "[Epoch: 23]   [loss avg: 7.4766]   [current loss: 0.1885]\n",
      "[Epoch: 24]   [loss avg: 7.2214]   [current loss: 0.1414]\n",
      "[Epoch: 25]   [loss avg: 6.9811]   [current loss: 0.1455]\n",
      "[Epoch: 26]   [loss avg: 6.7561]   [current loss: 0.1398]\n",
      "[Epoch: 27]   [loss avg: 6.5446]   [current loss: 0.1376]\n",
      "[Epoch: 28]   [loss avg: 6.3475]   [current loss: 0.1559]\n",
      "[Epoch: 29]   [loss avg: 6.1610]   [current loss: 0.0962]\n",
      "[Epoch: 30]   [loss avg: 5.9844]   [current loss: 0.1092]\n",
      "[Epoch: 31]   [loss avg: 5.8191]   [current loss: 0.0815]\n",
      "[Epoch: 32]   [loss avg: 5.6604]   [current loss: 0.0814]\n",
      "[Epoch: 33]   [loss avg: 5.5104]   [current loss: 0.1013]\n",
      "[Epoch: 34]   [loss avg: 5.3683]   [current loss: 0.0831]\n",
      "[Epoch: 35]   [loss avg: 5.2333]   [current loss: 0.0903]\n",
      "[Epoch: 36]   [loss avg: 5.1051]   [current loss: 0.0765]\n",
      "[Epoch: 37]   [loss avg: 4.9832]   [current loss: 0.0736]\n",
      "[Epoch: 38]   [loss avg: 4.8664]   [current loss: 0.0694]\n",
      "[Epoch: 39]   [loss avg: 4.7550]   [current loss: 0.0627]\n",
      "[Epoch: 40]   [loss avg: 4.6481]   [current loss: 0.0564]\n",
      "[Epoch: 41]   [loss avg: 4.5459]   [current loss: 0.0588]\n",
      "[Epoch: 42]   [loss avg: 4.4480]   [current loss: 0.0522]\n",
      "[Epoch: 43]   [loss avg: 4.3542]   [current loss: 0.0481]\n",
      "[Epoch: 44]   [loss avg: 4.2648]   [current loss: 0.0416]\n",
      "[Epoch: 45]   [loss avg: 4.1790]   [current loss: 0.0479]\n",
      "[Epoch: 46]   [loss avg: 4.0964]   [current loss: 0.0412]\n",
      "[Epoch: 47]   [loss avg: 4.0174]   [current loss: 0.0534]\n",
      "[Epoch: 48]   [loss avg: 3.9413]   [current loss: 0.0566]\n",
      "[Epoch: 49]   [loss avg: 3.8676]   [current loss: 0.0357]\n",
      "[Epoch: 50]   [loss avg: 3.7970]   [current loss: 0.0402]\n",
      "[Epoch: 51]   [loss avg: 3.7292]   [current loss: 0.0475]\n",
      "[Epoch: 52]   [loss avg: 3.6634]   [current loss: 0.0470]\n",
      "[Epoch: 53]   [loss avg: 3.5998]   [current loss: 0.0365]\n",
      "[Epoch: 54]   [loss avg: 3.5384]   [current loss: 0.0364]\n",
      "[Epoch: 55]   [loss avg: 3.4793]   [current loss: 0.0269]\n",
      "[Epoch: 56]   [loss avg: 3.4222]   [current loss: 0.0332]\n",
      "[Epoch: 57]   [loss avg: 3.3667]   [current loss: 0.0312]\n",
      "[Epoch: 58]   [loss avg: 3.3130]   [current loss: 0.0280]\n",
      "[Epoch: 59]   [loss avg: 3.2614]   [current loss: 0.0346]\n",
      "[Epoch: 60]   [loss avg: 3.2109]   [current loss: 0.0281]\n",
      "[Epoch: 61]   [loss avg: 3.1622]   [current loss: 0.0253]\n",
      "[Epoch: 62]   [loss avg: 3.1151]   [current loss: 0.0399]\n",
      "[Epoch: 63]   [loss avg: 3.0691]   [current loss: 0.0263]\n",
      "[Epoch: 64]   [loss avg: 3.0249]   [current loss: 0.0253]\n",
      "[Epoch: 65]   [loss avg: 2.9816]   [current loss: 0.0249]\n",
      "[Epoch: 66]   [loss avg: 2.9395]   [current loss: 0.0302]\n",
      "[Epoch: 67]   [loss avg: 2.8989]   [current loss: 0.0363]\n",
      "[Epoch: 68]   [loss avg: 2.8594]   [current loss: 0.0267]\n",
      "[Epoch: 69]   [loss avg: 2.8205]   [current loss: 0.0211]\n",
      "[Epoch: 70]   [loss avg: 2.7828]   [current loss: 0.0215]\n",
      "[Epoch: 71]   [loss avg: 2.7461]   [current loss: 0.0295]\n",
      "[Epoch: 72]   [loss avg: 2.7107]   [current loss: 0.0271]\n",
      "[Epoch: 73]   [loss avg: 2.6758]   [current loss: 0.0170]\n",
      "[Epoch: 74]   [loss avg: 2.6420]   [current loss: 0.0193]\n",
      "[Epoch: 75]   [loss avg: 2.6092]   [current loss: 0.0259]\n",
      "[Epoch: 76]   [loss avg: 2.5770]   [current loss: 0.0253]\n",
      "[Epoch: 77]   [loss avg: 2.5458]   [current loss: 0.0234]\n",
      "[Epoch: 78]   [loss avg: 2.5152]   [current loss: 0.0174]\n",
      "[Epoch: 79]   [loss avg: 2.4854]   [current loss: 0.0191]\n",
      "[Epoch: 80]   [loss avg: 2.4562]   [current loss: 0.0183]\n",
      "[Epoch: 81]   [loss avg: 2.4278]   [current loss: 0.0224]\n",
      "[Epoch: 82]   [loss avg: 2.4000]   [current loss: 0.0203]\n",
      "[Epoch: 83]   [loss avg: 2.3727]   [current loss: 0.0160]\n",
      "[Epoch: 84]   [loss avg: 2.3461]   [current loss: 0.0163]\n",
      "[Epoch: 85]   [loss avg: 2.3202]   [current loss: 0.0186]\n",
      "[Epoch: 86]   [loss avg: 2.2947]   [current loss: 0.0154]\n",
      "[Epoch: 87]   [loss avg: 2.2700]   [current loss: 0.0172]\n",
      "[Epoch: 88]   [loss avg: 2.2458]   [current loss: 0.0131]\n",
      "[Epoch: 89]   [loss avg: 2.2220]   [current loss: 0.0133]\n",
      "[Epoch: 90]   [loss avg: 2.1990]   [current loss: 0.0170]\n",
      "[Epoch: 91]   [loss avg: 2.1763]   [current loss: 0.0171]\n",
      "[Epoch: 92]   [loss avg: 2.1540]   [current loss: 0.0168]\n",
      "[Epoch: 93]   [loss avg: 2.1322]   [current loss: 0.0159]\n",
      "[Epoch: 94]   [loss avg: 2.1109]   [current loss: 0.0118]\n",
      "[Epoch: 95]   [loss avg: 2.0900]   [current loss: 0.0127]\n",
      "[Epoch: 96]   [loss avg: 2.0695]   [current loss: 0.0154]\n",
      "[Epoch: 97]   [loss avg: 2.0495]   [current loss: 0.0191]\n",
      "[Epoch: 98]   [loss avg: 2.0297]   [current loss: 0.0114]\n",
      "[Epoch: 99]   [loss avg: 2.0104]   [current loss: 0.0129]\n",
      "[Epoch: 100]   [loss avg: 1.9915]   [current loss: 0.0108]\n",
      "[Epoch: 101]   [loss avg: 1.9729]   [current loss: 0.0191]\n",
      "[Epoch: 102]   [loss avg: 1.9546]   [current loss: 0.0118]\n",
      "[Epoch: 103]   [loss avg: 1.9367]   [current loss: 0.0122]\n",
      "[Epoch: 104]   [loss avg: 1.9191]   [current loss: 0.0147]\n",
      "[Epoch: 105]   [loss avg: 1.9018]   [current loss: 0.0103]\n",
      "[Epoch: 106]   [loss avg: 1.8848]   [current loss: 0.0108]\n",
      "[Epoch: 107]   [loss avg: 1.8682]   [current loss: 0.0106]\n",
      "[Epoch: 108]   [loss avg: 1.8519]   [current loss: 0.0180]\n",
      "[Epoch: 109]   [loss avg: 1.8358]   [current loss: 0.0134]\n",
      "[Epoch: 110]   [loss avg: 1.8200]   [current loss: 0.0130]\n",
      "[Epoch: 111]   [loss avg: 1.8045]   [current loss: 0.0172]\n",
      "[Epoch: 112]   [loss avg: 1.7892]   [current loss: 0.0116]\n",
      "[Epoch: 113]   [loss avg: 1.7742]   [current loss: 0.0095]\n",
      "[Epoch: 114]   [loss avg: 1.7594]   [current loss: 0.0125]\n",
      "[Epoch: 115]   [loss avg: 1.7449]   [current loss: 0.0083]\n",
      "[Epoch: 116]   [loss avg: 1.7307]   [current loss: 0.0136]\n",
      "[Epoch: 117]   [loss avg: 1.7167]   [current loss: 0.0104]\n",
      "[Epoch: 118]   [loss avg: 1.7029]   [current loss: 0.0118]\n",
      "[Epoch: 119]   [loss avg: 1.6893]   [current loss: 0.0097]\n",
      "[Epoch: 120]   [loss avg: 1.6761]   [current loss: 0.0100]\n",
      "[Epoch: 121]   [loss avg: 1.6629]   [current loss: 0.0109]\n",
      "[Epoch: 122]   [loss avg: 1.6499]   [current loss: 0.0090]\n",
      "[Epoch: 123]   [loss avg: 1.6372]   [current loss: 0.0106]\n",
      "[Epoch: 124]   [loss avg: 1.6246]   [current loss: 0.0085]\n",
      "[Epoch: 125]   [loss avg: 1.6123]   [current loss: 0.0126]\n",
      "[Epoch: 126]   [loss avg: 1.6001]   [current loss: 0.0081]\n",
      "[Epoch: 127]   [loss avg: 1.5881]   [current loss: 0.0084]\n",
      "[Epoch: 128]   [loss avg: 1.5763]   [current loss: 0.0103]\n",
      "[Epoch: 129]   [loss avg: 1.5647]   [current loss: 0.0108]\n",
      "[Epoch: 130]   [loss avg: 1.5533]   [current loss: 0.0124]\n",
      "[Epoch: 131]   [loss avg: 1.5420]   [current loss: 0.0080]\n",
      "[Epoch: 132]   [loss avg: 1.5309]   [current loss: 0.0067]\n",
      "[Epoch: 133]   [loss avg: 1.5200]   [current loss: 0.0094]\n",
      "[Epoch: 134]   [loss avg: 1.5091]   [current loss: 0.0079]\n",
      "[Epoch: 135]   [loss avg: 1.4985]   [current loss: 0.0073]\n",
      "[Epoch: 136]   [loss avg: 1.4880]   [current loss: 0.0119]\n",
      "[Epoch: 137]   [loss avg: 1.4777]   [current loss: 0.0085]\n",
      "[Epoch: 138]   [loss avg: 1.4675]   [current loss: 0.0070]\n",
      "[Epoch: 139]   [loss avg: 1.4575]   [current loss: 0.0109]\n",
      "[Epoch: 140]   [loss avg: 1.4476]   [current loss: 0.0080]\n",
      "[Epoch: 141]   [loss avg: 1.4378]   [current loss: 0.0090]\n",
      "[Epoch: 142]   [loss avg: 1.4282]   [current loss: 0.0089]\n",
      "[Epoch: 143]   [loss avg: 1.4187]   [current loss: 0.0130]\n",
      "[Epoch: 144]   [loss avg: 1.4093]   [current loss: 0.0095]\n",
      "[Epoch: 145]   [loss avg: 1.4000]   [current loss: 0.0104]\n",
      "[Epoch: 146]   [loss avg: 1.3909]   [current loss: 0.0072]\n",
      "[Epoch: 147]   [loss avg: 1.3819]   [current loss: 0.0102]\n",
      "[Epoch: 148]   [loss avg: 1.3730]   [current loss: 0.0066]\n",
      "[Epoch: 149]   [loss avg: 1.3642]   [current loss: 0.0087]\n",
      "[Epoch: 150]   [loss avg: 1.3556]   [current loss: 0.0075]\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0   1.000000  0.536585  0.698413        41\n",
      "         1.0   0.719882  0.757977  0.738438      1285\n",
      "         2.0   0.813296  0.769746  0.790922       747\n",
      "         3.0   0.987013  0.356808  0.524138       213\n",
      "         4.0   0.792000  0.910345  0.847059       435\n",
      "         5.0   0.917614  0.983257  0.949302       657\n",
      "         6.0   1.000000  0.040000  0.076923        25\n",
      "         7.0   0.995360  0.997674  0.996516       430\n",
      "         8.0   0.000000  0.000000  0.000000        18\n",
      "         9.0   0.711520  0.840000  0.770440       875\n",
      "        10.0   0.848382  0.901357  0.874068      2210\n",
      "        11.0   0.784314  0.599251  0.679406       534\n",
      "        12.0   0.935484  0.940541  0.938005       185\n",
      "        13.0   0.951407  0.979807  0.965398      1139\n",
      "        14.0   0.872951  0.613833  0.720812       347\n",
      "        15.0   0.947368  0.428571  0.590164        84\n",
      "\n",
      "    accuracy                       0.835230      9225\n",
      "   macro avg   0.829787  0.665984  0.697500      9225\n",
      "weighted avg   0.838997  0.835230  0.828620      9225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import spectral\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# 用于测试样本的比例\n",
    "test_ratio = 0.90\n",
    "# 每个像素周围提取 patch 的尺寸\n",
    "patch_size = 15\n",
    "name = \"IP\"\n",
    "if name == 'PU':\n",
    "    class_num = 9\n",
    "if name == 'SA':\n",
    "    class_num = 16\n",
    "if name == 'IP':\n",
    "    class_num = 16\n",
    "################################下载数据集################################\n",
    "#wget.download('http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat', 'Indian_pines_corrected.mat')\n",
    "#wget.download('http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat', 'Indian_pines_gt.mat')\n",
    "\n",
    "################################(class)模型的具体操作步骤################################\n",
    "\n",
    "class CNN_1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on Paper:Chen, Y. Deep Feature Extraction and Classification of Hyperspectral Images Based on Convolutional Neural Networks. TGRS\n",
    "    input shape:[N,C=1,L=spectral_channel]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        #self.dataset = dataset\n",
    "        self.C2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=6, kernel_size=(3,)),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.C4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=6, out_channels=12, kernel_size=(3,)),\n",
    "            nn.BatchNorm1d(12),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.C6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=12, out_channels=24, kernel_size=(3,)),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.C8 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=24, out_channels=48, kernel_size=(3,)),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),  # for IP ,p=0.3, else(PU and SV) is 0.1\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(21504,128),\n",
    "            nn.Dropout(p=0.4),\n",
    "        )\n",
    "        self.classifier = nn.Linear(128,class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print(x.shape)\n",
    "        #x = torch.squeeze(x, dim=2).transpose(1, 2)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1]*x.shape[2]*x.shape[3])\n",
    "       # print(x.shape)\n",
    "        C2 = self.C2(x)\n",
    "        #print(C2.shape)\n",
    "        C4 = self.C4(C2)\n",
    "        #print(C4.shape)\n",
    "        last = self.C6(C4)\n",
    "        #print(last.shape)\n",
    "        last = self.C8(last)\n",
    "        #print(last.shape)\n",
    "        last = torch.reshape(last, (last.shape[0], -1))\n",
    "        #print(last.shape)\n",
    "        FC = self.fc(last)\n",
    "        #print(FC.shape)\n",
    "        out = self.classifier(FC)\n",
    "        return out\n",
    "\n",
    "\n",
    "################################数据处理################################\n",
    "# 对高光谱数据 X 应用 PCA 变换\n",
    "def applyPCA(X, numComponents):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
    "    return newX\n",
    "\n",
    "# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "# # 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n",
    "# def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):#X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n",
    "#     # 给 X 做 padding\n",
    "#     margin = int((windowSize - 1) / 2)\n",
    "#     zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "#     # split patches\n",
    "#     patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))##(145*145，25，25，30)\n",
    "#     patchesLabels = np.zeros((X.shape[0] * X.shape[1]))#(145*145)\n",
    "#     patchIndex = 0\n",
    "#     for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "#         for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "#             patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "#             patchesData[patchIndex, :, :, :] = patch\n",
    "#             patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "#             patchIndex = patchIndex + 1\n",
    "#     if removeZeroLabels:\n",
    "#         patchesData = patchesData[patchesLabels > 0, :, :, :]\n",
    "#         patchesLabels = patchesLabels[patchesLabels > 0]\n",
    "#         patchesLabels -= 1\n",
    "#     return patchesData, patchesLabels\n",
    "# 在每个像素周围提取 patch\n",
    "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    # 给 X 做 padding\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # 获得 y 中的标记样本数\n",
    "    count = 0\n",
    "    for r in range(0, y.shape[0]):\n",
    "        for c in range(0, y.shape[1]):\n",
    "            if y[r, c] != 0:\n",
    "                count = count+1\n",
    "\n",
    "    # split patches\n",
    "    patchesData = np.zeros([count, windowSize, windowSize, X.shape[2]])\n",
    "    patchesLabels = np.zeros(count)\n",
    "\n",
    "    count = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            if y[r-margin, c-margin] != 0:\n",
    "                patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "                patchesData[count, :, :, :] = patch\n",
    "                patchesLabels[count] = y[r-margin, c-margin]\n",
    "                count = count + 1\n",
    "\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "#类别\n",
    "#class_num = 16\n",
    "\n",
    "#X = sio.loadmat('C:\\AI\\data\\Indian_pines_corrected.mat')['indian_pines_corrected']#X是数据集\n",
    "#y = sio.loadmat('C:\\AI\\data\\Indian_pines_gt.mat')['indian_pines_gt']#y是标签\n",
    "\n",
    "def loadData(name):\n",
    "    data_path = os.path.join(os.getcwd(), 'mnt')\n",
    "    if name == 'IP':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
    "    elif name == 'SA':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "    elif name == 'PU':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
    "    else:\n",
    "        print(\"NO DATASET\")\n",
    "        exit()\n",
    "    return data, labels\n",
    "\n",
    "X, y = loadData(name)\n",
    "\n",
    "\n",
    "# 使用 PCA 降维，得到主成分的数量\n",
    "if name == 'PU':\n",
    "    pca_components = 32\n",
    "if name == 'SA':\n",
    "    pca_components = 32\n",
    "if name == 'IP':\n",
    "    pca_components = 32\n",
    "#pca_components = 15\n",
    "\n",
    "print('Hyperspectral data shape: ', X.shape)##(145,145,200)\n",
    "print('Label shape: ', y.shape)#(145,145)\n",
    "\n",
    "print('\\n... ... PCA tranformation ... ...')\n",
    "\n",
    "X_pca = applyPCA(X, numComponents=pca_components)\n",
    "print('Data shape after PCA: ', X_pca.shape)#(145,145,30)\n",
    "\n",
    "print('\\n... ... create data cubes ... ...')\n",
    "X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)#(10249,25,25,30)    (10249)\n",
    "print('Data cube X shape: ', X_pca.shape)#(10249,25,25,30)\n",
    "print('Data cube y shape: ', y.shape)#(10249)\n",
    "\n",
    "print('\\n... ... create train & test data ... ...')\n",
    "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n",
    "print('Xtrain shape: ', Xtrain.shape)#(1024, 25, 25, 30)\n",
    "print('Xtest  shape: ', Xtest.shape)#(9225, 25, 25, 30)\n",
    "\n",
    "# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n",
    "Xtrain = Xtrain.reshape(-1, pca_components, patch_size, patch_size)\n",
    "Xtest  = Xtest.reshape(-1, pca_components, patch_size, patch_size)\n",
    "print('before transpose: Xtrain shape: ', Xtrain.shape)#(1024, 25, 25, 30, 1)\n",
    "print('before transpose: Xtest  shape: ', Xtest.shape)#(9225, 25, 25, 30, 1)\n",
    "\n",
    "# # 为了适应 pytorch 结构，数据要做 transpose\n",
    "# Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n",
    "# Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n",
    "# print('after transpose: Xtrain shape: ', Xtrain.shape)#(1024, 1, 30, 25, 25)\n",
    "# print('after transpose: Xtest  shape: ', Xtest.shape)#(9225, 1, 30, 25, 25)\n",
    "\n",
    "\n",
    "\"\"\" Training dataset\"\"\"\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtrain.shape[0]\n",
    "        self.x_data = torch.FloatTensor(Xtrain)\n",
    "        self.y_data = torch.LongTensor(ytrain)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtest.shape[0]\n",
    "        self.x_data = torch.FloatTensor(Xtest)\n",
    "        self.y_data = torch.LongTensor(ytest)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "testset  = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "one = torch.ones(128, dtype=torch.long).to(device)\n",
    "two = torch.ones(76, dtype=torch.long).to(device)\n",
    "# 网络放到GPU上\n",
    "net = CNN_1D().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "#开始训练\n",
    "net.train()\n",
    "total_loss = 0\n",
    "for epoch in range(150):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        try:\n",
    "            labels = labels - one\n",
    "        except:\n",
    "            labels = labels - two\n",
    "\n",
    "        #print(labels)\n",
    "        # 优化器梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 正向传播 +　反向传播 + 优化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
    "\n",
    "print('Finished Training')\n",
    "path = 'model' + '.pth'\n",
    "torch.save(net, path)\n",
    "\n",
    "if name == 'PU':\n",
    "    t = int(42776 * test_ratio)+1   #10249 42776 54129\n",
    "if name == 'SA':\n",
    "    t = int(54129 * test_ratio) + 1  # 10249 42776 54129\n",
    "if name == 'IP':\n",
    "    t = int(10249 * test_ratio) + 1  # 10249 42776 54129\n",
    "#print(t)\n",
    "a = np.ones(t)##9225是Xtest.shape[0]\n",
    "ytest = ytest - a\n",
    "\n",
    "count = 0\n",
    "# 模型测试\n",
    "net.eval()\n",
    "for inputs, _ in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = net(inputs)\n",
    "    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "    if count == 0:\n",
    "        y_pred_test =  outputs\n",
    "        count = 1\n",
    "    else:\n",
    "        y_pred_test = np.concatenate((y_pred_test, outputs))\n",
    "\n",
    "# 生成分类报告\n",
    "classification = classification_report(ytest, y_pred_test, digits=6)\n",
    "print(classification)\n",
    "\n",
    "from operator import truediv\n",
    "\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc\n",
    "\n",
    "\n",
    "def reports(test_loader, y_test, name):\n",
    "    count = 0\n",
    "    # 模型测试\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        if count == 0:\n",
    "            y_pred = outputs\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred = np.concatenate((y_pred, outputs))\n",
    "\n",
    "    if name == 'IP':\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "            , 'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',\n",
    "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                        'Stone-Steel-Towers']\n",
    "    elif name == 'SA':\n",
    "        target_names = ['Brocoli_green_weeds_1', 'Brocoli_green_weeds_2', 'Fallow', 'Fallow_rough_plow',\n",
    "                        'Fallow_smooth',\n",
    "                        'Stubble', 'Celery', 'Grapes_untrained', 'Soil_vinyard_develop', 'Corn_senesced_green_weeds',\n",
    "                        'Lettuce_romaine_4wk', 'Lettuce_romaine_5wk', 'Lettuce_romaine_6wk', 'Lettuce_romaine_7wk',\n",
    "                        'Vinyard_untrained', 'Vinyard_vertical_trellis']\n",
    "    elif name == 'PU':\n",
    "        target_names = ['Asphalt', 'Meadows', 'Gravel', 'Trees', 'Painted metal sheets', 'Bare Soil', 'Bitumen',\n",
    "                        'Self-Blocking Bricks', 'Shadows']\n",
    "\n",
    "    classification = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    return classification, confusion, oa * 100, each_acc * 100, aa * 100, kappa * 100\n",
    "\n",
    "\n",
    "# 将结果写在文件里\n",
    "classification, confusion, oa, each_acc, aa, kappa = reports(test_loader, ytest, name)\n",
    "classification = str(classification)\n",
    "confusion = str(confusion)\n",
    "file_name = \"classification_report.txt\"\n",
    "\n",
    "# with open(file_name, 'w') as x_file:\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('{} Overall accuracy (%)'.format(oa))\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('{} Average accuracy (%)'.format(aa))\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('{}'.format(classification))\n",
    "#     x_file.write('\\n')\n",
    "#     x_file.write('{}'.format(confusion))\n",
    "# 显示结果\n",
    "# load the original image\n",
    "\n",
    "\n",
    "# X, y = loadData(name)\n",
    "# height = y.shape[0]\n",
    "# width = y.shape[1]\n",
    "\n",
    "# X = applyPCA(X, numComponents=pca_components)\n",
    "# X = padWithZeros(X, patch_size // 2)\n",
    "\n",
    "# # 逐像素预测类别\n",
    "# outputs = np.zeros((height, width))\n",
    "# for i in range(height):\n",
    "#     for j in range(width):\n",
    "#         if int(y[i, j]) == 0:\n",
    "#             continue\n",
    "#         else:\n",
    "#             image_patch = X[i:i + patch_size, j:j + patch_size, :]\n",
    "#             #print(image_patch.shape)\n",
    "#             image_patch = image_patch.reshape(1, image_patch.shape[0], image_patch.shape[1], image_patch.shape[2])\n",
    "#             #print(image_patch.shape)\n",
    "#             image_patch = image_patch.reshape(1, pca_components, patch_size, patch_size)\n",
    "#             X_test_image = torch.FloatTensor(image_patch).to(device)\n",
    "#             #print(X_test_image.shape)\n",
    "#             prediction = net(X_test_image)\n",
    "#             prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n",
    "#             outputs[i][j] = prediction + 1\n",
    "#     if i % 20 == 0:\n",
    "#         print('... ... row ', i, ' handling ... ...')\n",
    "\n",
    "# oringal_image = spectral.imshow(classes=y, figsize=(7, 7))\n",
    "# predict_image = spectral.imshow(classes=outputs.astype(int), figsize=(7, 7))\n",
    "# spectral.save_rgb(name + \"2D-原始.jpg\", y.astype(int), colors=spectral.spy_colors)\n",
    "# spectral.save_rgb(name + \"2D-预测.jpg\", outputs.astype(int), colors=spectral.spy_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
